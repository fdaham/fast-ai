{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will cover an approach to the [Cats vs. Dogs Kaggle Competition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition). Along the way, we will dig deeper into the architecture of a neural network. This includes learning how weights are initialized and optimized as well as the role of activation functions in neural networks. We will also finetune a pre-trained VGG 16 model and build a linear model in Keras.\n",
    "\n",
    "Before we get to any of that, here are a few steps needed to enter a Kaggle Competition..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps are needed to get an account, install Kaggle-CLI, and enter a competition:\n",
    "\n",
    "1. **Create a Kaggle account.** You'll need a username and password when entering your competition submissions, so get an account that isn't linked to any social media. If you do opt to connect to a social media account, you can choose a username and password linked to your account by clicking \"forgot password\".\n",
    "2. **Install Kaggle-CLI.** SSH into your instance, then enter: \n",
    "<br><br>\n",
    "```pip install kaggle-cli```\n",
    "<br><br>\n",
    "3. **Enter a competition.** The competition name is \"dogs-vs-cats-redux-kernels-edition\".\n",
    "<br><br>\n",
    "```kg config -g -u username -p password -c competition```\n",
    "<br><br>\n",
    "4. **Accept the Competition's Terms and Conditions.** This can be done on the competition's Kaggle page. Attempting to download data for the competition before completing this step will yield an error. \n",
    "5. **Download the data.** Now that you've entered into the competition and accepted the terms and conditions, you can download the necessary data sets (train and test directories):\n",
    "<br><br>\n",
    "```kg download```\n",
    "<br><br>\n",
    "To unzip the folders, install ```unzip``` and unzip each file:\n",
    "<br><br>\n",
    "```sudo apt install unzip```\n",
    "<br>\n",
    "```unzip *.zip```\n",
    "<br><br>\n",
    "6. **Enter submission.** This is the last step. Your submission should be saved as a CSV file.\n",
    "<br><br>\n",
    "```kg submit submission -c competition -u username -p password -m “Enter submission description here.”```\n",
    "<br>\n",
    "\n",
    "Now that we've entered the competition and downloaded the necessary data sets, we can carry on with our submission. One thing to note here is the competition's evaluation metric, **log loss**.\n",
    "\n",
    "### Log Loss\n",
    "\n",
    "In information theory, binary entropy (a subset of categorical cross-entropy) is defined as the measure of uncertainty of the Bernoulli process with probability of success $y$. When considering the total distribution of the data set, the summation of the entropy must be included. This is how submissions are scored on the log loss, as defined [here](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition#evaluation):\n",
    "\n",
    "\\begin{equation*}\n",
    "LogLoss =  -\\frac{1}{n} \\sum_{i=1}^n y_ilog(\\hat{y_i}) + (1-y_i)log(1-\\hat{y_i})\n",
    "\\end{equation*}\n",
    "\n",
    "where $n$ is the number of images in the training set, $\\hat{y_i}$ is the predicted probability of the image being of a dog, and $y_i$ is the true label of whether the image is of a dog (1) or cat (0).\n",
    "\n",
    "As you may have already noticed, probability values of 0 or 1 will give undefined loss. Kaggle accomodates for this by offsetting zeros and ones by a very small amount. Therefore, by the way that it is calculated, log loss will yield extremely low for predictions that are confident and correct (p = 0.9999, label = 1) and drastically increase for those that are confident and incorrect (p = 0.0001, label = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhEAAAGHCAYAAAAOSQDRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f111e137450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "x = [i*.0001 for i in range(1,10000)]\n",
    "y = [log_loss([1],[[i*.0001,1-(i*.0001)]],eps=1e-15) for i in range(1,10000,1)]\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.axis([-.05, 1.1, -.8, 10])\n",
    "plt.title(\"Log Loss (True Label = 1)\")\n",
    "plt.xlabel(\"Predicted Probability\")\n",
    "plt.ylabel(\"Log Loss\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this issue, we can clip the range to lessen the log loss. This is done in my [Cats vs. Dogs Competition notebook](https://github.com/fdaham/fastai/blob/master/dogs_cats_kaggle.ipynb).\n",
    "\n",
    "Now, back to CNNs..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Understanding CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We used a Convolutional Neural Network (CNN) in Lesson 1 to classify images of cats and dogs. But, what is a neural network? Vaguely put, it is a universal approximator used for architectures that have a high tolerance to error (which is why we use it for image recognition). Neural networks are, essentially, a collection, or series, of matrix products where an input is mapped to an output through matrix multiplication. \n",
    "\n",
    "For CNNs specifically, inputs (taken from training sets) are just arrays of pixel values. We map our input to weights (initialized randomly and later trained) in order to yield activation matrices, which we try to get as close as possible to our desired output through minimizing our loss function (a measure of how well our model fits empirical data). This is just one layer of the network. Below is an example of a network with just one input layer (first column), hidden layer (second column), and output (courtesy of [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)):\n",
    "\n",
    "![image](http://i.imgur.com/Yz4Znws.png[/img)\n",
    "\n",
    "One of the algorithms used in this network to minimize our loss function and optimize weights is called **gradient descent**. \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent uses initialized parameter values and iteratively takes the partial derivative of the loss function with respect to each parameter to minimize the loss. The parameters are updated by taking steps (i.e. a **learning rate**) in the direction opposite to the partial derivatives of the loss function. This process is done over the entire training data set, which exceeds the computational limits of a neural network.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is possibly the most important algorithm to learn in deep learning. Instead of evaluating the loss function over the entire training data set, predictions are made on random data samples (or **mini batches**) and their true values are used to evaluate the loss function. This helps to avoid the compute-heavy issue that arises when using standard gradient descent.    \n",
    "\n",
    "Here's an interesting question asked during lecture: what happens when you evaluate the loss function and reach a local minimum where the derivative is zero? This could almost never happen in a deep learning network as there are hundreds of millions of dimensions. The chance of each dimension sharing the same minimum is nearly impossible. \n",
    "\n",
    "After one random mini batch pass, our weights are updated and we can proceed to the next mini batch. So, how are the weights updated?\n",
    "\n",
    "### Weight Optimization\n",
    "\n",
    "As previously mentioned, **weight optimization** takes all the weights of the filters of a network and updates them in the opposite direction of the gradient. Keras, like most deep learning libraries, handles weight optimization for us, so we don't need to worry about weight initialization. However, **weight initialization** is important because, if weights are set to be too small or too large, their signals will either shrink or blow up through each layer of the network to the point where they become useless. Either case will inevitably happen when randomly initializing weights (done for symmetry purposes) for each filter in the network. To avoid this issue, we set our initial weight values using a method called **Xavier Initialization**. \n",
    "\n",
    "We use Xavier Initialization to initialize our weights to adequately scale our output. We start by setting our initial weight matrices, $W_{i}$, to a distribution (typically Gaussian or uniform) with a mean of zero and variance, $V$, of: \n",
    "\n",
    "\\begin{equation*}\n",
    "V(W_{i}) = \\frac{1}{n_{in}}\n",
    "\\end{equation*}\n",
    "\n",
    "where $n_{in}$ is the number of input vectors making up the $W_{i}$ initialization distribution. You can set the dimensions of the weight matrices to be anything, but you’re given an input and output matrix. Therefore, the dimensions of the input matrix should match that of the output matrix. Note: if input $A$ is an $n×m$ matrix and ouput $B$ is an $m×p$ matrix, $AB$ is an $n×p$ matrix where $n$, $m$, $p$  are all positive integers. Here's a short summary of the derivation:\n",
    "\n",
    "Our output matrix ($Y$) is the dot product of our weight ($W$) and input ($X$) matrices:\n",
    "\n",
    "\\begin{equation*}\n",
    "Y = W_{1}X_{1} + W_{2}X_{2} + ... + W_{n}X_{n}\n",
    "\\end{equation*}\n",
    " \n",
    "The variance of the input is $Var(X_{i}) = E[X_{i}^{2}] + {E[X_{i}]}^2$, where $E[X_{i}]$ is the expected value of the squared deviation from the mean. Similarly, the variance of the weight matrix is $Var(W_{i}) = E[W_{i}^{2}] + {E[W_{i}]}^2$. $W$ and $X$ are independent of one another, so the variance of their dot product is:\n",
    "\n",
    "\\begin{equation*}\n",
    "Var(W_{i}X_{i}) = {[E(W_{i})]}^{2}Var(X_{i}) + {[E(X_{i})]}^{2}Var(W_{i}) + Var(W_{i})Var(X_{i})\n",
    "\\end{equation*}\n",
    "\n",
    "If our inputs and weights both have a zero mean, our combined variance simplifies to:\n",
    "\n",
    "\\begin{equation*}\n",
    "Var(W_{i}X_{i}) = Var(W_{i})Var(X_{i})\n",
    "\\end{equation*}\n",
    "\n",
    "So, the variance of our output is:\n",
    "\n",
    "\\begin{equation*}\n",
    "Var(Y) = Var(W_{1}X_{1} + W_{2}X_{2} + ... + W_{n}X_{n}) = nVar(W_{i})Var(X_{i}) \n",
    "\\end{equation*}\n",
    "\n",
    "We want the variance of our input and output to be the same, so this simplifies to $nVar(W_{i}) = 1$, or: \n",
    "\n",
    "\\begin{equation*}\n",
    "Var(W_{i}) = \\frac{1}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, *$n = n_{in}$*. $V(W_{i})$ can also be derived from backpropagation (training multiple layers of a model--introduced later in this lesson), which gives $n_{in} = n_{out}$. Taking the average of the two yields:\n",
    "\n",
    "\\begin{equation*}\n",
    "V(W_{i}) = \\frac{2}{n_{in} + n_{out}}\n",
    "\\end{equation*}  \n",
    "\n",
    "Now that we know how to initialize our weights, how do we update (or optimize) them for each filter in our network? This is done by minimizing our loss function with respect to each weight parameter. You can visualize the loss below (image taken from [a Beginner's Guide to Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/), a post that I found very helpful):\n",
    "\n",
    "![img](http://i.imgur.com/seZGfcn.png[/img])\n",
    "\n",
    "This is just a simple example of how loss varies as a function of two weights. To minimize loss, *$L$*, the weights must be updated by taking the derivative of the loss function with respect to both parameters and taking steps in the direction opposite the gradient:\n",
    "\n",
    "\\begin{equation*}\n",
    "W = W_{in} - l \\frac{dL}{dW}\n",
    "\\end{equation*}\n",
    "\n",
    "where *$l$* is the learning rate--a parameter we optimally set. The learning rate allows the model to converge on an optimal set of weights in a short amount of time. However, setting the rate too high could result in large steps that may never converge to the optimal point.\n",
    "\n",
    "Now that we know how to initialize and optimize our weights through the SGD algorithm, let's test what we know by building a simple linear model in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Building a Linear Model\n",
    "\n",
    "Here, we'll build a simple linear model trained using the 1,000 predictions from the Imagenet model, where all the images are our inputs and the labels (dog or cat) are our target outputs. Let's import the necessary libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import utils; reload(utils)\n",
    "from utils import plots, get_batches, plot_confusion_matrix, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each dense layer, as we've discussed in the previous lesson, is a linear model followed by an activation function (which we'll discuss more later). A **linear model** is a model where each row (or layer) is calculated as the sum of the dot product of the row and weights. The weights are learned from the data and the same weights are mapped to each row. \n",
    "\n",
    "Let's observe how a linear model works by seeing how Keras implements gradient descent in the context of linear regression. To do this, we need data that we know is linearly related. In this example, we will be using $y(x) = 2x_{1} + 3x_{2} + 1$, where the input is $x = [x_{1} x_{2}]$ and the output is $y$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = random((30,2))\n",
    "y = np.dot(x, [2., 3.]) + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we can create our simple linear model, ```lm```, in Keras. Here, we pass our inputs and expected outputs (```x``` and ```y```) so that Keras can initialize some form of random weights. The model is then optimized using SGD to minimize our loss function--in this case, **Mean Squared Error (```mse```)**--with a learning rate of 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential([ Dense(1, input_shape=(2,)) ])\n",
    "lm.compile(optimizer=SGD(lr=0.1), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The weights have now been trained. The next step is to evaluate our loss. Obviously, we're looking for a very small number here... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.739252090454102"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we expected... To iteratively pass our ```x``` and ```y``` pairs through the SGD algorithm to evaluate the loss function and update the parameters, we'll call the ```fit``` function. We run this through 5 epochs before re-evaluating our predictions. Notice how the loss is minimized after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s - loss: 1.8141     \n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s - loss: 0.1805     \n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s - loss: 0.0959     \n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s - loss: 0.0472     \n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s - loss: 0.0297     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1110dfd990>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x, y, nb_epoch=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015679962933063507"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That works! For confirmation, we can see that our weights have been optimized so that, after fitting, they're close to the weights we used to calculate ```y``` (2.0, 3.0, and 1.0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.6216],\n",
       "        [ 2.7918]], dtype=float32), array([ 1.3504], dtype=float32)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Linear Model\n",
    "\n",
    "So far, we've trained a linear model from a pre-trained model's output. Now, we can apply what we've learned to the Cats vs Dogs Kaggle competition. That is, we want to train a simple linear model to take the 1,000 predictions (learned from Kaggle data) as inputs and return the *dog* or *cat* labels as outputs. To do this, let's start with a few configuration steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the max batch size is a matter of trial and error. Here, we're able to use a batch size of 100 to start for our VGG 16 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by grabbing our training and validation batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "batches = get_batches(path+'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll load and save the preprocessed arrays using ```bcolz``` to avoid loading and resizing the images every time we use them. Using ```bcolz``` to save and load arrays simplifies the space and time complexity of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the arrays in all the batches for the validation and training sets and load them so that we don't have to recalculate them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23000 images belonging to 2 classes.\n",
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn_data = get_data(path+'train')\n",
    "val_data = get_data(path+'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_data.bc', trn_data)\n",
    "save_array(model_path+'valid_data.bc', val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that I don't have enough RAM to load the ```trn_data``` array into memory. So, instead, ```bcolz.open()``` was used to mmap (or memory-map) the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_data = bcolz.open(model_path+'train_data.bc', mode='r')\n",
    "val_data = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classes are returned by Keras in an array. Here, we convert them to multi-dimensional arrays for one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1,000 Imagenet predictions from VGG16 will be the features for our linear model. We can also load these features to avoid recalculating them in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "val_features = model.predict(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_lastlayer_features.bc', trn_features)\n",
    "save_array(model_path+'valid_lastlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_lastlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_lastlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define our linear model, just like we did earlier, and fit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1000 inputs (saved features) and 2 outputs (dog and cat)\n",
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0975 - acc: 0.9673 - val_loss: 0.0885 - val_acc: 0.9730\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0780 - acc: 0.9752 - val_loss: 0.0926 - val_acc: 0.9740\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0789 - acc: 0.9769 - val_loss: 0.0968 - val_acc: 0.9745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f10ea1b5690>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(trn_features, trn_labels, nb_epoch=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve better results through a process called **finetuning**, as we've seen in the previous lesson. Before we get to that, we need to introduce **activation layers** and their role in neural networks.\n",
    "\n",
    "### Activation Layers\n",
    "\n",
    "As discussed before, neural networks are a series of matrix multiplications (or layers) that transform an input vector. If a network were to only be a sequence of matrix multiplications, it would not pass linear complexity and result in just a single matrix. But, this isn't the case for deep learning models. Deep learning model layers have nonlinear functions operated on the output from the previous layer. The resulting vector then becomes the input to the next layer, called **activation layers**. Every layer in VGG16 has an activation function, which tells Keras how to transform the output of a linear layer. Some common activation functions include tanh, sigmoid (```1/(1+exp(x))```), and rectified linear unit (ReLU, ```max(0,x)```), which is the most common activation function). \n",
    "\n",
    "We know activation functions exist in neural networks, but why? They help introduce nonlinearity to our network. ReLU is the most common activation function used in these networks because it has little effect on the accuracy of our model; we're just replacing all of our negative values with 0. \n",
    "\n",
    "The final layer of our model generally needs a different activation function to better represent our final output. For example, if our expected output is a one-hot encoded parameter, we want the values of our output vector to sum to one, where one value (or probability, since it's out of 1) is significantly larger than the rest. This way, we convert the greatest number to 1 and the rest to 0. The activation function that does this is **softmax**, defined as ```exp(x[i])/sum(exp(x))```.\n",
    "\n",
    "#### Modifying the Model\n",
    "\n",
    "Looking at the last layer of the VGG16 model, we can see that it's a dense layer that outputs 1,000 elements. Therefore, it seems redundant to stack another dense layer meant to find cats and dogs on top of one that's meant to find Imagenet categories. In that sense, we're limiting information by forcing the network to classify Imagenet categories before labeling our images as either being a cat or dog. So, let's just remove that last layer of the model. We'll train the model like before, but now we'll be removing the last layer and telling Keras to adjust the weights accordingly for the other layers since we aren't looking to learn new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138357544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we removed the last layer, let's add our new final activation layer (```softmax```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to compile our updated model and randomize our training batches to use the preprocessed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a simple function for our fitting models and use it to train our ```softmax``` layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 655s - loss: 0.3866 - acc: 0.9748 - val_loss: 0.3398 - val_acc: 0.9785\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 656s - loss: 0.3653 - acc: 0.9763 - val_loss: 0.3229 - val_acc: 0.9795\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always want to save our weights for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 54s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.32286706900597312, 0.97950000000000004]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now successfully fine-tuned our new final layer and can do this for all of our dense layers, using the chain rule to calculate our gradients through a method called **backpropagation**.\n",
    "\n",
    "### Introduction to Backpropagation\n",
    "\n",
    "Stacking linear and non-linear (activation) layers is simply defining a convolution of functions. Each layer takes the output of the previous layer and uses it as its input. Therefore, we can calculate the derivative at any layer by simply multiplying the gradients of that layer and all of its following layers together. This use of the chain rule to allow us to rapidly calculate the derivatives of our model at any layer is referred to as backpropagation. Libraries like Theano, Tensorflow, and Keras already do this for us through automatic differentiation (AD). \n",
    "\n",
    "We can now apply this method to train the other layers of our network.\n",
    "\n",
    "#### Training Multiple Layers in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We haven't changed our model, so there's no need to re-compile it, we just need to update the learning rate. Since we're training more layers and we've already optimized the last one, we should use a smaller rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 659s - loss: 0.3213 - acc: 0.9793 - val_loss: 0.2992 - val_acc: 0.9810\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 658s - loss: 0.3437 - acc: 0.9776 - val_loss: 0.2978 - val_acc: 0.9805\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 656s - loss: 0.3415 - acc: 0.9780 - val_loss: 0.3104 - val_acc: 0.9800\n"
     ]
    }
   ],
   "source": [
    "K.set_value(opt.lr, 0.01)\n",
    "fit_model(model, batches, val_batches, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have now fine-tuned all of our dense layers--all optimized for our data set. Notice how our training accuracy has increased using this method. Finally, let's save the weights for this updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune2.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
