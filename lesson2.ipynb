{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 2: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we will cover an approach to the [Cats vs. Dogs Kaggle Competition](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition). Along the way, we will dig deeper into the architecture of a neural network. This includes learning how weights are initialized and optimized as well as the role of activation functions in neural networks. We will also finetune a pre-trained VGG16 model and build a linear model in Keras.\n",
    "\n",
    "Before we get into all of that, here are a few steps needed to enter a Kaggle Competition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the steps needed to start your Kaggle account, install Kaggle-CLI, and enter into a competition:\n",
    "\n",
    "1. **Get a Kaggle account.** You'll need a username and password when entering your competition submissions in Terminal, so get an account that isn't linked to any social media. If you do opt to connect to a social media account, you can choose a username and password linked to your account by clicking \"forgot password\".\n",
    "2. **SSH into your instance and install Kaggle-CLI.** \n",
    "<br><br>\n",
    "```pip install kaggle-cli```\n",
    "<br><br>\n",
    "3. **Enter into the competition.** The competition name is *dogs-vs-cats-redux-kernels-edition*:\n",
    "<br><br>\n",
    "```kg config -g -u username -p password -c competition```\n",
    "<br><br>\n",
    "4. **Accept the Competition's Terms and Conditions.** This can be done on the competition's Kaggle page. Downloading data for the competition without doing this step will yield an error. \n",
    "5. **Download the data sets.** Now that you've entered into the competition and accepted the terms and conditions, you can download the necessary data (train and test directories):\n",
    "<br><br>\n",
    "```kg download```\n",
    "<br><br>\n",
    "To unzip the folders, install *unzip* and unzip each file:\n",
    "<br><br>\n",
    "```sudo apt install unzip```\n",
    "<br>\n",
    "```unzip *.zip```\n",
    "<br>\n",
    "\n",
    "Now that we've entered the competition and downloaded the necessary data sets, we can carry on with our submission. This is done in my [Cats vs. Dogs Competition notebook](google.com). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing and Understanding Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We used a Convolutional Neural Network (CNN) in Lesson 1 to classify images of cats and dogs and we know it's a type of neural network (duh). But what is a neural network? Vaguely put, it is a universal approximator used for architectures that have a high tolerance to error (which is why we use it for image recognition). Neural networks are, essentially, a collection, or series, of matrix products where an input is mapped to an output through matrix multiplication. \n",
    "\n",
    "For CNNs specifically, inputs (taken from training sets) are just arrays of pixel values. We map our input to weights (initialized randomly and later trained) in order to yield activation matrices, which we try to get as close as possible to our desired output through minimizing our loss function (a measure of how well our model fits empirical data). This is just one layer of the network. Below is an example of a network with just one input layer (first column), hidden layer (second column), and output (courtesy of [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)):\n",
    "\n",
    "![image](http://i.imgur.com/Yz4Znws.png[/img)\n",
    "\n",
    "One of the algorithms used in this network to minimize our loss function and optimize weights in order to get as close to our desired output as possible is called **gradient descent**. \n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent uses initialized parameter values and iteratively takes the partial derivative of the loss function with respect to each parameter to minimize the loss. The parameters are updated by taking steps (i.e. a **learning rate**) in the opposite direction of the partial derivatives of the loss function. This process is done over the entire training data set, which exceeds the computational limits of a neural network.\n",
    "\n",
    "**Stochastic Gradient Descent (SGD)** is possibly the most important algorithm to learn in deep learning. Instead of evaluating the loss function over the entire training data set, predictions are made on random data samples (or **mini batches**) and their true values are used to evaluate the loss function. This helps avoid the compute-heavy issue that arises when using standard gradient descent.    \n",
    "\n",
    "Here's an interesting question asked during lecture: what happens when you evaluate the loss function and reach a local minimum where the derivative is zero? This could almost never happen in a deep learning network as there are hundreds of millions of dimensions. The chance of each dimension sharing the same minimum is nearly impossible. \n",
    "\n",
    "After one random mini batch pass, our weights are updated and we can proceed to the next mini batch. So, how are the weights updated?\n",
    "\n",
    "### Weight Optimization\n",
    "\n",
    "As previously mentioned, **weight optimization** takes all the weights of the filters of a network and updates them in the opposite direction of the gradient. Keras, like most deep learning libraries, handles all weight optimization for us so that we don't even need to worry about weight initialization. However, **weight initialization** is important because, if weights are set to be too small or too large, their signals will either shrink or blow up through each layer of the network to the point where they become useless. Either case will inevitably happen when randomly initializing weights (done for symmetry purposes) for each filter in the network. To avoid this issue, we set our initial weight values using a method called **Xavier Initialization**. \n",
    "\n",
    "Before we delve into weight optimization between layers of a network, we must first initialize our weights to adequately scale our output. For this, we use Xavier Initialization: setting our initial weight matrices, *$W_{i}$*, to a distribution (typically Gaussian or uniform) with a mean of zero and variance, *V*, of: \n",
    "\n",
    "\\begin{equation*}\n",
    "V(W_{i}) = \\frac{1}{n_{in}}\n",
    "\\end{equation*}\n",
    "\n",
    "where $n_{in}$ is the number of input vectors making up the *$W_{i}$* initialization distribution. You can set the dimensions of the weight matrices to be anything, but you’re given an input and output matrix. Therefore, the dimensions of our input matrix should match that of the output matrix. Note: if input *A* is an *n×m* matrix and ouput *B* is an *m×p* matrix, *AB* is an *n×p* matrix where *n*, *m*, *p*  are all positive integers. Here's a short summary of the derivation:\n",
    "\n",
    "> Our output matrix (*Y*) is the dot product of our weight (*W*) and input (*X*) matrices:\n",
    ">\n",
    ">\\begin{equation*}\n",
    "Y = W_{1}X_{1} + W_{2}X_{2} + ... + W_{n}X_{n}\n",
    "\\end{equation*}\n",
    "> \n",
    "> The variance of the input is $Var(X_{i}) = E[X_{i}^{2}] + {E[X_{i}]}^2$, where $E[X_{i}]$ is the expected value of the squared deviation from the mean. Similarly, the variance of the weight matrix is $Var(W_{i}) = E[W_{i}^{2}] + {E[W_{i}]}^2$. *W* and *X* are independent of one another, so the variance of their dot product is:\n",
    ">\n",
    "> \\begin{equation*}\n",
    "Var(W_{i}X_{i}) = {[E(W_{i})]}^{2}Var(X_{i}) + {[E(X_{i})]}^{2}Var(W_{i}) + Var(W_{i})Var(X_{i})\n",
    "\\end{equation*}\n",
    ">\n",
    "> If our inputs and weights both have a zero mean, our combined variance simplifies to:\n",
    ">\n",
    "> \\begin{equation*}\n",
    "Var(W_{i}X_{i}) = Var(W_{i})Var(X_{i})\n",
    "\\end{equation*}\n",
    ">\n",
    "> So, the variance of our output is:\n",
    ">\n",
    "> \\begin{equation*}\n",
    "Var(Y) = Var(W_{1}X_{1} + W_{2}X_{2} + ... + W_{n}X_{n}) = nVar(W_{i})Var(X_{i}) \n",
    "\\end{equation*}\n",
    ">\n",
    "> We want the variance of our input and output to be the same, so this simplifies to $nVar(W_{i}) = 1$, or: \n",
    ">\n",
    "> \\begin{equation*}\n",
    "Var(W_{i}) = \\frac{1}{n}\n",
    "\\end{equation*}\n",
    "\n",
    "Here, n = $n_{in}$. $V(W_{i})$ can also be derived from back propagation (training multiple layers of a model--introduced later in this lesson), which gives $n_{in} = n_{out}$. Taking the average of the two yields:\n",
    "\n",
    "\\begin{equation*}\n",
    "V(W_{i}) = \\frac{2}{n_{in} + n_{out}}\n",
    "\\end{equation*}  \n",
    "\n",
    "Now that we know how to initialize our weights, how do we update (or optimize) them for each filter in our network? This is done by minimizing our loss function with respect to each weight parameter. You can visualize the loss below (image taken from [a Beginner's Guide to Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/), a post that I found very helpful):\n",
    "\n",
    "![img](http://i.imgur.com/seZGfcn.png[/img])\n",
    "\n",
    "This is just a simple example of how loss varies as a function of two weights. To minimize loss (*L*), the weights must be updated by taking the derivative of the loss function with respect to both parameters and taking steps in the direction opposite to the gradient:\n",
    "\n",
    "\\begin{equation*}\n",
    "W = W_{in} - l \\frac{dL}{dW}\n",
    "\\end{equation*}\n",
    "\n",
    "where *l* is the learning rate--a parameter we optimally set. Our learning rate will take less time for the model to converge on an optimal set of weights. However, setting the rate too high could result in large steps that may never converge to the optimal point.\n",
    "\n",
    "Now that we know how to initial and optimize our weights through the SGD algorithm, let's test what we know by building a simple linear model in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Building a Linear Model in Keras\n",
    "\n",
    "Here, we will build a simple linear model trained using the 1,000 predictions from the Imagenet model, where all the images are our inputs and the labels (dog or cat) are our target outputs. Let's import the necessary libraries first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division,print_function\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "from matplotlib import pyplot as plt\n",
    "import utils; reload(utils)\n",
    "from utils import plots, get_batches, plot_confusion_matrix, get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import random, permutation\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Each ***Dense() layer*** is just a linear model followed by an activation function (which we'll get into more later). A **linear model** is a model where each row is calculated as the sum of the dot product of the row and weights. The weights are learned from the data and the same weight is mapped to each row. Let's see how a linear model works by showing how Keras implements gradient descent in the context of linear regression. To do this, we need data that we know is linearly related. In this example, we will be using $y = 2x_{1} + 3x_{2} + 1$, where $x = [x_{1} x_{2}]$ is input and *y* is expected output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = random((30,2))\n",
    "y = np.dot(x, [2., 3.]) + 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we can create our simple linear model in Keras. Here, we pass our inputs and expected outputs (*x* and *y*) so that Keras can initialize some form of random weights. The model is then optimized using SGD to minimize our loss function--in this case, **Mean Squared Error** (MSE)--with a learning rate of 0.1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lm = Sequential([ Dense(1, input_shape=(2,)) ])\n",
    "lm.compile(optimizer=SGD(lr=0.1), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The weights have now been trained. The next step is to evaluate our loss (we're obviously looking for a very small number here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): We did not found a dynamic library into the library_dir of the library we use for blas. If you use ATLAS, make sure to compile it with dynamics library.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.6175813674926758"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not what we expected... To iteratively pass our *x* and *y* pairs through the SGD algorithm to evaluate the loss function and update the parameters, we'll call **fit()**. We run this through 5 epochs before re-evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 0s - loss: 0.0076     \n",
      "Epoch 2/5\n",
      "30/30 [==============================] - 0s - loss: 0.0035     \n",
      "Epoch 3/5\n",
      "30/30 [==============================] - 0s - loss: 0.0016     \n",
      "Epoch 4/5\n",
      "30/30 [==============================] - 0s - loss: 7.8622e-04     \n",
      "Epoch 5/5\n",
      "30/30 [==============================] - 0s - loss: 4.0045e-04     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35c0523d90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(x, y, nb_epoch=5, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0002334274904569611"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.evaluate(x, y, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That works! For confirmation, we can see that our weights have been optimized so that, after fitting, they're close to the weights we used to calculate *y* (2.0, 3.0, and 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.9724],\n",
       "        [ 2.954 ]], dtype=float32), array([ 1.0374], dtype=float32)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Linear Model\n",
    "\n",
    "The method of simply attaching dense layers to pre-trained models to make predictions is how many deep learning amateurs achieve good results. What we've done so far is astonishingly simple; we trained a linear model from a pre-trained model's output. Now, we can apply what we've learned so far to the Cats vs Dogs Kaggle competition. That is, we want to train a simple linear model to take the 1,000 predictions (learned from Kaggle data) as inputs and return the *dog* or *cat* labels as outputs. To do this, let's start with a few configuration steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"data/dogscats/\"\n",
    "model_path = path + 'models/'\n",
    "if not os.path.exists(model_path): os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the max batch size is a matter of trial and error. Here, we're able to use a batch size of 100 to start with for our VGG 16 model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()\n",
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, this is what needs to be done:\n",
    "\n",
    "1. Get the labels for each image.\n",
    "2. Get the 1,000 Imagenet category predictions for each image.\n",
    "3. Feed these predictions as input to a simple linear model.\n",
    "\n",
    "Let's start by grabbing our training and validation batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 23000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# use batch size of 1 since we're just doing preprocessing on the CPU\n",
    "val_batches = get_batches(path+'valid', shuffle=False, batch_size=1)\n",
    "batches = get_batches(path+'train', shuffle=False, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll load and save the preprocessed arrays using *bcolz* to avoid loading and resizing the images every time we use them. Using *bcolz* to save and load numpy arrays simplifies the space and time complexity of our network by compressing the arrays (our first step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "def save_array(fname, arr): c=bcolz.carray(arr, rootdir=fname, mode='w'); c.flush()\n",
    "def load_array(fname): return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join the arrays in all the batches for the validation and training sets and load them so that we don't have to recalculate them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = get_data(path+'valid')\n",
    "trn_data = get_data(path+'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_data.bc', trn_data)\n",
    "save_array(model_path+'valid_data.bc', val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_data = load_array(model_path+'train_data.bc')\n",
    "val_data = load_array(model_path+'valid_data.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classes are returned by Keras in an array. Here, we convert them to multi-dimensional arrays for one hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def onehot(x): return np.array(OneHotEncoder().fit_transform(x.reshape(-1,1)).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_classes = val_batches.classes\n",
    "trn_classes = batches.classes\n",
    "val_labels = onehot(val_classes)\n",
    "trn_labels = onehot(trn_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1,000 Imagenet predictions from VGG16 will be the features for our linear model. We can also load these features to avoid recalculating them in the future (our second step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_features = model.predict(trn_data, batch_size=batch_size)\n",
    "val_features = model.predict(val_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(model_path+'train_lastlayer_features.bc', trn_features)\n",
    "save_array(model_path+'valid_lastlayer_features.bc', val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trn_features = load_array(model_path+'train_lastlayer_features.bc')\n",
    "val_features = load_array(model_path+'valid_lastlayer_features.bc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define our linear model, just like we did earlier, and fit it (our third step):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1000 inputs, since that's the saved features, and 2 outputs, for dog and cat\n",
    "lm = Sequential([ Dense(2, activation='softmax', input_shape=(1000,)) ])\n",
    "lm.compile(optimizer=RMSprop(lr=0.1), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23000 samples, validate on 2000 samples\n",
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0787 - acc: 0.9782 - val_loss: 0.1070 - val_acc: 0.9755\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0798 - acc: 0.9782 - val_loss: 0.1161 - val_acc: 0.9730\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 0s - loss: 0.0803 - acc: 0.9791 - val_loss: 0.1129 - val_acc: 0.9770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f35a84ddb90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.fit(trn_features, trn_labels, nb_epoch=3, batch_size=batch_size, \n",
    "       validation_data=(val_features, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve better results through a process called **fine-tuning**. Before we get into that, we need to introduce **activation layers** and their role in neural networks.\n",
    "\n",
    "### Activation Layers\n",
    "\n",
    "As discussed before, neural networks are a series of matrix multiplications (or layers) that transform an input vector. If a network were to only be a sequence of matrix multiplications, it would not pass linear complexity and result in just a single matrix. But, this isn't the case for deep learning models. Deep learning model layers have nonlinear functions operated on the output from the previous layer. The resulting vector then becomes the input to the next layer, which are **activation layers**. Every layer in VGG16 has an activation function, which tells Keras how to transform the output of a linear layer. Some common activation functions include tanh, sigmoid ($1/(1+exp(x))$), and rectified linear unit (**ReLU**, $max(0,x)$, which is the most common activation function). \n",
    "\n",
    "We know activation functions exist in neural networks, but why? This helps introduce nonlinearity to our network. ReLU is the most common activation function used in these networks because it has little effect on the accuracy of our model, we're just replacing all of our negative values with 0. \n",
    "\n",
    "The final layer of our model generally needs a different activation function to better represent our final output. For example, if our expected output is a one hot encoded parameter, we want the values of our output vector to sum to one, where one value (or probability, since it's out of 1) is significantly larger than the rest. This way, we convert the greatest number to 1 and the rest to 0. The activation function that does this is **softmax**, defined as $exp(x[i]) / sum(exp(x))$.\n",
    "\n",
    "#### Modifying the Model using Fine-Tuning\n",
    "\n",
    "Looking at the last layer of the VGG16 model, we can see that it's a dense layer that outputs 1000 elements. Therefore, it seems redundant to stack another dense layer meant to find cats and dogs on top of one that's meant to find Imagenet categories. In that sense, we're limiting information by forcing the network to classify Imagenet categories before labeling our images as either being a cat or dog. So, let's just remove that last layer of the model. We'll train the model like before, but now we'll be removing the last layer and telling Keras to adjust the weights accordingly for the other layers since we aren't looking to learn new parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 4096)          0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 4096)          0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 138357544\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we removed the last layer, let's add our new final activation layer (softmax):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "??vgg.finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to compile our updated model and randomize our training batches to use the preprocessed images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator()\n",
    "batches = gen.flow(trn_data, trn_labels, batch_size=batch_size, shuffle=True)\n",
    "val_batches = gen.flow(val_data, val_labels, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a simple function for our fitting models and use it to train our softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, batches, val_batches, nb_epoch=1):\n",
    "    model.fit_generator(batches, samples_per_epoch=batches.N, nb_epoch=nb_epoch, \n",
    "                        validation_data=val_batches, nb_val_samples=val_batches.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = RMSprop(lr=0.1)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "23000/23000 [==============================] - 273s - loss: 0.7262 - acc: 0.9520 - val_loss: 0.3888 - val_acc: 0.9750\n",
      "Epoch 2/2\n",
      "23000/23000 [==============================] - 272s - loss: 0.6278 - acc: 0.9601 - val_loss: 0.4218 - val_acc: 0.9735\n"
     ]
    }
   ],
   "source": [
    "fit_model(model, batches, val_batches, nb_epoch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We always want to save our weights for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'finetune1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 23s    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.42180404308438973, 0.97350000000000003]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now successfully fine-tuned our new final layer and can do this for all of our dense layers, using the chain rule to calculate our gradients through a method called **back propagation**.\n",
    "\n",
    "### Introduction to Back Propagation\n",
    "\n",
    "Stacking linear and non-linear (activation) layers is simply defining a convolution of functions. Each layer takes the output of the previous layer and uses it as its input. Therefore, we can calculate the derivative at any layer by simply multiplying the gradients of that layer and all of its following layers together. This use of the chain rule to allow us to rapidly calculate the derivatives of our model at any layer is referred to as back propagation. Libraries like Theano, Tensorflow, and Keras already do this for us through automatic differentiation (AD). \n",
    "\n",
    "We can now apply this method to train the other layers of our network.\n",
    "\n",
    "#### Training Multiple Layers in Keras with Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layers = model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index,layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: layer.trainable=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We haven't changed our model, so there's no need to re-compile it, we just update the learning rate. Since we're training more layers and we've already optimized the last one, we should use a smaller rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "23000/23000 [==============================] - 272s - loss: 0.4665 - acc: 0.9703 - val_loss: 0.4522 - val_acc: 0.9710\n",
      "Epoch 2/3\n",
      "23000/23000 [==============================] - 273s - loss: 0.4468 - acc: 0.9716 - val_loss: 0.3980 - val_acc: 0.9740\n",
      "Epoch 3/3\n",
      "23000/23000 [==============================] - 272s - loss: 0.4187 - acc: 0.9736 - val_loss: 0.3903 - val_acc: 0.9745\n"
     ]
    }
   ],
   "source": [
    "K.set_value(opt.lr, 0.01)\n",
    "fit_model(model, batches, val_batches, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have now fine-tuned all of our dense layers--all optimized for our data set. Let's save the weights again for this updated model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'finetune2.h5')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "nbpresent": {
   "slides": {
    "28b43202-5690-4169-9aca-6b9dabfeb3ec": {
     "id": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "prev": null,
     "regions": {
      "3bba644a-cf4d-4a49-9fbd-e2554428cf9f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f3d3a388-7e2a-4151-9b50-c20498fceacc",
        "part": "whole"
       },
       "id": "3bba644a-cf4d-4a49-9fbd-e2554428cf9f"
      }
     }
    },
    "8104def2-4b68-44a0-8f1b-b03bf3b2a079": {
     "id": "8104def2-4b68-44a0-8f1b-b03bf3b2a079",
     "prev": "28b43202-5690-4169-9aca-6b9dabfeb3ec",
     "regions": {
      "7dded777-1ddf-4100-99ae-25cf1c15b575": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fe47bd48-3414-4657-92e7-8b8d6cb0df00",
        "part": "whole"
       },
       "id": "7dded777-1ddf-4100-99ae-25cf1c15b575"
      }
     }
    }
   },
   "themes": {}
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
