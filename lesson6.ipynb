{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6: Building RNNs\n",
    "\n",
    "In this lesson, we will first review what we've learned about pseudo labeling and embeddings from [Lesson 4](https://github.com/fdaham/fastai/blob/master/lesson4.ipynb) and [Lesson 5](https://github.com/fdaham/fastai/blob/master/lesson5.ipynb). Then, we will cover a few RNN architectures before building one from scratch in Theano.\n",
    "\n",
    "## Reviewing Pseudo Labeling and Embeddings\n",
    "\n",
    "### Psuedo Labeling\n",
    "\n",
    "Pseudo labeling allows us to learn more information from unlabeled data (especially when there are large amounts of it) in conjunction with labeled data. To do this, our model uses labeled data to train and make predictions on our test set. Then, another model is built using data from the training set and the pseudo labeled test set in ratios of 2:1 or 3:1 (labeled:unlabeled data). Keras doesn't have a built in function to generate batches from different data sets, so we use ```MixIterator```, a class written by Jeremy, to do this:  \n",
    "\n",
    "```python\n",
    "class MixIterator(object):\n",
    "    def __init__(self, iters):\n",
    "        self.iters = iters\n",
    "        self.N = sum([it.N for it in self.iters])\n",
    "        \n",
    "    def reset(self):\n",
    "        for it in self.iters: it.reset()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "        \n",
    "    def next(self, *args, **kwargs):\n",
    "        nexts = [next(it) for it in self.iters]\n",
    "        n0 = np.concatenate([n(0) for n in nexts])\n",
    "        n1 = np.concatenate([n[1] for n in nexts])\n",
    "        return (n0, n1)\n",
    "```\n",
    "\n",
    "### Embeddings\n",
    "\n",
    "In Lessons 4 and 5, we used embeddings to represent user movie ratings and trained our model to optimize these parameters through gradient descent. In Keras, the embedding matrices were passed to functions for both users and movies to generate embeddings for user/movie IDs. The given user movie ratings (raw data) were used as our third input, or the target outputs of our model. Based on the calculated loss function, the embeddings were updated. For that example, we used matrices to list each element of our embeddings (for learning purposes). However, representing them this way is impratical as there are limited numbers of user/movie combinations, leaving their matrices mostly sparse. \n",
    "\n",
    "Now, back to RNNs...\n",
    "\n",
    "## RNNs\n",
    "\n",
    "We know RNNs are best used to keep track of memory. Let's try to build an architecture that best reflects this use; keeping track of past states to predict future ones. The example we briefly discussed in Lesson 5 shows how a RNN uses state to guess the next word in a sentence. The network (shown in the diagram below) takes in an embedding for a word, passes it through two layers of transformations, then combines it with the transformed embedding of the next word. This is what gives the network a sense of state. At the final layer, a prediction for the last word in the sentence is made. Therefore, the final word is dependent on the information learned from the preceeding words.   \n",
    "\n",
    "### Building a 4-Character Model\n",
    "\n",
    "We'll now be taking advantage of Keras' functional API to construct arbitrary architectures. Let's consider a network that predicts the fourth character from inputting the first three: \n",
    "\n",
    "![img](https://i.imgur.com/DaZbuXZ.png[/img])\n",
    "\n",
    "Every <font color='green'>green</font> arrow (input to hidden layer) is performing the same action. They are all essentially weight matrices with equal dimensionality. The <font color='orange'>orange</font> arrows take the hidden state from the previous char and combine them with the hidden state of the next char. The <font color='blue'>blue</font> arrow predicts the fourth char, given a hidden state. Therefore, it's fair to assume that the task for transforming and concatenating chars 1 and 2 is the same as for chars 3 and 4, making these tasks **time shift invariant** (of course time, $t$, being sequential in nature). \n",
    "\n",
    "Let's start by downloading all necessary libraries and completing a few configuration steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n",
      "WARNING (theano.sandbox.cuda): Ignoring call to use(1), GPU number 0 is already in use.\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda\n",
    "cuda.use('gpu1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n"
     ]
    }
   ],
   "source": [
    "# downloading Nietzsche's collected works\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 86\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# add 0 for padding\n",
    "chars.insert(0, \"\\0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "char_indices = dict((c, i) for i, c in enumerate(chars)) # map from chars to inds\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars)) # map from inds to chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# create array of all indices for characters in corpus\n",
    "idx = [char_indices[c] for c in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are now ready to build our model. Ultimately, we want to predict the fourth character from a sequence of three. Let's start by creating a list of every fourth character in ```idx``` (our numerical character representation of the text), starting with characters 0-3. Characters 0-2 are our inputs (```x1```, ```x2```, and ```x3```) and they are stacked into seperate numpy arrays, followed by character 3 (out output, ```y```):   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cs = 3\n",
    "c1_dat = [idx[i] for i in xrange(0, len(idx)-1-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in xrange(0, len(idx)-1-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in xrange(0, len(idx)-1-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in xrange(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat[:-2])\n",
    "x2 = np.stack(c2_dat[:-2])\n",
    "x3 = np.stack(c3_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat[:-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Next, we'll create an embedding for each input character, where each embedding contains 42 latent factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_fac = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    emb = Embedding(n_in, n_out, input_length=1)(inp)\n",
    "    return inp, Flatten()(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c1_in, c1 = embedding_input('c1', vocab_size, n_fac)\n",
    "c2_in, c2 = embedding_input('c2', vocab_size, n_fac)\n",
    "c3_in, c3 = embedding_input('c3', vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We'll now focus on our input's path through the hidden state (the <font color='green'>green</font> arrow in the diagram above). ```n_hidden``` is the size of our hidden state, which we chose to be 256, and ```dense_in``` is the layer operation. Our first hidden activation, ```c1_hidden```, is the output generated from the embedding of our first character. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c1_hidden = dense_in(c1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now, we can define the layer operation from hidden to hidden (the <font color='orange'>orange</font> arrows in the diagram above). This sets us up to build the merging portions of our network using these two layers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dense_hidden = Dense(n_hidden, activation='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c2_dense = dense_in(c2)\n",
    "hidden_2 = dense_hidden(c1_hidden)\n",
    "c2_hidden = merge([c2_dense, hidden_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c3_dense = dense_in(c3)\n",
    "hidden_3 = dense_hidden(c2_hidden)\n",
    "c3_hidden = merge([c3_dense, hidden_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we can define the layer operation from hidden to output (the <font color='blue'>blue</font> arrow in the diagram). Here, our third hidden state is transformed into our final output prediction. We use this to build and train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c4_out = dense_out(c3_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Model([c1_in, c2_in, c3_in], c4_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.optimizer.lr=0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the default learning rate to train the model. After just four epochs, our loss is 2.1662. Originally, when the learning rate was set to a very small number (```model.optimizer.lr = 0.000001```) and tested to generate the fourth word in the sequence, ```get_next``` would only return ' '. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "200297/200297 [==============================] - 13s - loss: 2.4017    \n",
      "Epoch 2/4\n",
      "200297/200297 [==============================] - 13s - loss: 2.2617    \n",
      "Epoch 3/4\n",
      "200297/200297 [==============================] - 13s - loss: 2.2112    \n",
      "Epoch 4/4\n",
      "200297/200297 [==============================] - 13s - loss: 2.1662    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f48071e7510>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([x1, x2, x3], y, batch_size=64, nb_epoch=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "Let's now test our model. We'll first need the function ```get_next```, which takes in a string argument, makes an array of index values for every character in the input (like we did earlier for the first three characters of the text), stacks the input into a numpy array, then returns the char with the maximum index value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = model.predict(arrs)\n",
    "    i = np.argmax(p)\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our model, we'll use the following character sequences: 'phi', ' th', and '  an'. The model is limited because it only makes predictions based off of the preceeding three characters, but does a pretty good job at completing the words 'phil', 'the', and 'and'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'l'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('phi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next(' an')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an *N*-Character Model\n",
    "\n",
    "The diagram above shows the unrolled version of a network that takes in the first three characters as input to predict the fourth character output. This time, we want to build a more arbitrary model that can predict the *n*th character given a sequence of *n*-1 characters. To do this, we'll stack all the input/hidden layers from our previous model *n*-1 times. In the diagram below, we can see this new recurrent network:\n",
    "\n",
    "![img](https://i.imgur.com/JnwzwIJ.png[/img])\n",
    "\n",
    "**Note:** when stacking Keras on top of Tensorflow, RNNs can only be implemented in their unrolled form. However, Theano can implement RNNs in their recurrent form (actions boxed in <font color='red'>red</font> in the above diagram), which we will use for this build.\n",
    "\n",
    "We will now build our *n*th character RNN (where ```cs = n-1```). In this example, let's say we want to predict the 9th character in a sequence of 8 (```cs = 8```). The input matrices, ```c_in_dat```, and desired output, ```c_out_dat```, are created and stacked in seperate numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+n] for i in xrange(0, len(idx)-1-cs, cs)]\n",
    "            for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [idx[i+cs] for i in xrange(0, len(idx)-1-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xs = [np.stack(c[:-2]) for c in c_in_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_fac = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "Similar to our 4-character model, we need to create the embeddings for each character and define our layer tasks. This time, we're keeping track of more characters. Therefore, we should expect our model to yield better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name+'_in')\n",
    "    emb = Embedding(n_in, n_out, input_length=1, name=name+'_emb')(inp)\n",
    "    return inp, Flatten()(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_ins = [embedding_input('c'+str(n), vocab_size, n_fac) for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', init='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hidden = dense_in(c_ins[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(1,cs):\n",
    "    c_dense = dense_in(c_ins[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = merge([c_dense, hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_out = dense_out(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's now create and test our model, as we've done before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Model([c[0] for c in c_ins], c_out)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "75110/75110 [==============================] - 10s - loss: 2.5316    \n",
      "Epoch 2/12\n",
      "75110/75110 [==============================] - 10s - loss: 2.2511    \n",
      "Epoch 3/12\n",
      "75110/75110 [==============================] - 9s - loss: 2.1508     \n",
      "Epoch 4/12\n",
      "75110/75110 [==============================] - 9s - loss: 2.0820     \n",
      "Epoch 5/12\n",
      "75110/75110 [==============================] - 9s - loss: 2.0272     \n",
      "Epoch 6/12\n",
      "75110/75110 [==============================] - 9s - loss: 1.9812     \n",
      "Epoch 7/12\n",
      "75110/75110 [==============================] - 10s - loss: 1.9401    \n",
      "Epoch 8/12\n",
      "75110/75110 [==============================] - 10s - loss: 1.9044    \n",
      "Epoch 9/12\n",
      "75110/75110 [==============================] - 10s - loss: 1.8706    \n",
      "Epoch 10/12\n",
      "75110/75110 [==============================] - 10s - loss: 1.8410    \n",
      "Epoch 11/12\n",
      "75110/75110 [==============================] - 10s - loss: 1.8136    \n",
      "Epoch 12/12\n",
      "75110/75110 [==============================] - 10s - loss: 1.7889    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47fd47b790>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, y, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = [np.array(char_indices[c])[np.newaxis] for c in inp]\n",
    "    p = model.predict(idxs)\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "Our model has successfully predicted the last character given a list of the preceeding characters. This type of RNN is best suited for tasks like sentiment analysis, which uses sequences of chars/words as input. Keras has a built-in implementation of this RNN that we can use for our sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden, n_fac, cs, vocab_size = (256, 42, 8, 86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=cs),\n",
    "        SimpleRNN(n_hidden, activation='relu', inner_init='identity'),\n",
    "        Dense(vocab_size, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, 8, 42)         3612        embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_1 (SimpleRNN)          (None, 256)           76544       embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 86)            22102       simplernn_1[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 102258\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75110/75110 [==============================] - 10s - loss: 2.8023    \n",
      "Epoch 2/8\n",
      "75110/75110 [==============================] - 10s - loss: 2.2721    \n",
      "Epoch 3/8\n",
      "75110/75110 [==============================] - 10s - loss: 2.0549    \n",
      "Epoch 4/8\n",
      "75110/75110 [==============================] - 10s - loss: 1.9112    \n",
      "Epoch 5/8\n",
      "75110/75110 [==============================] - 10s - loss: 1.8078    \n",
      "Epoch 6/8\n",
      "75110/75110 [==============================] - 10s - loss: 1.7282    \n",
      "Epoch 7/8\n",
      "75110/75110 [==============================] - 9s - loss: 1.6636     \n",
      "Epoch 8/8\n",
      "75110/75110 [==============================] - 9s - loss: 1.6119     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47f73c2510>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.concatenate(xs,axis=1), y, batch_size=64, nb_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_next_keras(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = np.array(idxs)[np.newaxis,:]\n",
    "    p = model.predict(arrs)[0]\n",
    "    return chars[np.argmax(p)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_keras('this is ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'t'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_keras('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_keras('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that, so far, we've be initializing our hidden to hidden weight matrices as indentity matrices. For keeping track of state, this makes sense, however, hidden to hidden layer transformations are meant to give us information on how information from the previous state should be transformed before being combined with the newly transformed input. The best way to do this is to pass the information from the previous state directly to the contruction of our new one, optimized through SGD. \n",
    "\n",
    "### Building a 2 to *N* Character Sequence Model\n",
    "\n",
    "Let's now consider a model that returns sequences; instead of having our model predict the *n*th character from the preceeding *n-1* characters, let's have our model predict chars 2 to *n* from the preceeding *n*-1 characters. Referencing the previous computational diagram, our output is now included in our <font color='red'>red</font> highlighted iteration box (see diagram below). \n",
    "\n",
    "![img](https://i.imgur.com/kPFsJ0Q.png[/img])\n",
    "\n",
    "This model will increase the number of predictions we make on our training set. Instead of making only one prediction, we are making *n*-1 times that. Because of this, our model is able to handle more long-term memory tasks. To build this model, we need to change our output, ```c_out_dat```, to a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+n] for i in xrange(1, len(idx)-cs, cs)]\n",
    "            for n in range(cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ys = [np.stack(c[:-2]) for c in c_out_dat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "We can create and train our model as we've done before. However, instead of starting with an indentity matrix, we will first pass a zero vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dense_in = Dense(n_hidden, activation='relu')\n",
    "dense_hidden = Dense(n_hidden, activation='relu', init='identity')\n",
    "dense_out = Dense(vocab_size, activation='softmax', name='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "inp1 = Input(shape=(n_fac,), name='zeros')\n",
    "hidden = dense_in(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "outs = []\n",
    "\n",
    "for i in range(cs):\n",
    "    c_dense = dense_in(c_ins[i][1])\n",
    "    hidden = dense_hidden(hidden)\n",
    "    hidden = merge([c_dense, hidden], mode='sum')\n",
    "    # every layer now has an output\n",
    "    outs.append(dense_out(hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Model([inp1] + [c[0] for c in c_ins], outs)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "zeros = np.tile(np.zeros(n_fac), (len(xs[0]),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "75110/75110 [==============================] - 24s - loss: 20.1435 - output_loss_1: 2.7108 - output_loss_2: 2.5725 - output_loss_3: 2.5191 - output_loss_4: 2.4860 - output_loss_5: 2.4761 - output_loss_6: 2.4584 - output_loss_7: 2.4670 - output_loss_8: 2.4537    \n",
      "Epoch 2/12\n",
      "75110/75110 [==============================] - 24s - loss: 17.8743 - output_loss_1: 2.5135 - output_loss_2: 2.3545 - output_loss_3: 2.2413 - output_loss_4: 2.1744 - output_loss_5: 2.1583 - output_loss_6: 2.1410 - output_loss_7: 2.1587 - output_loss_8: 2.1326    \n",
      "Epoch 3/12\n",
      "75110/75110 [==============================] - 24s - loss: 17.2495 - output_loss_1: 2.4969 - output_loss_2: 2.3301 - output_loss_3: 2.1777 - output_loss_4: 2.0863 - output_loss_5: 2.0563 - output_loss_6: 2.0306 - output_loss_7: 2.0463 - output_loss_8: 2.0252    \n",
      "Epoch 4/12\n",
      "75110/75110 [==============================] - 23s - loss: 16.8557 - output_loss_1: 2.4898 - output_loss_2: 2.3221 - output_loss_3: 2.1449 - output_loss_4: 2.0308 - output_loss_5: 1.9895 - output_loss_6: 1.9561 - output_loss_7: 1.9703 - output_loss_8: 1.9521    \n",
      "Epoch 5/12\n",
      "75110/75110 [==============================] - 23s - loss: 16.5860 - output_loss_1: 2.4861 - output_loss_2: 2.3157 - output_loss_3: 2.1261 - output_loss_4: 1.9930 - output_loss_5: 1.9445 - output_loss_6: 1.9053 - output_loss_7: 1.9176 - output_loss_8: 1.8976    \n",
      "Epoch 6/12\n",
      "75110/75110 [==============================] - 23s - loss: 16.3906 - output_loss_1: 2.4842 - output_loss_2: 2.3122 - output_loss_3: 2.1132 - output_loss_4: 1.9655 - output_loss_5: 1.9091 - output_loss_6: 1.8670 - output_loss_7: 1.8799 - output_loss_8: 1.8596    \n",
      "Epoch 7/12\n",
      "75110/75110 [==============================] - 23s - loss: 16.2433 - output_loss_1: 2.4815 - output_loss_2: 2.3097 - output_loss_3: 2.1044 - output_loss_4: 1.9487 - output_loss_5: 1.8844 - output_loss_6: 1.8377 - output_loss_7: 1.8489 - output_loss_8: 1.8281    \n",
      "Epoch 8/12\n",
      "75110/75110 [==============================] - 24s - loss: 16.1234 - output_loss_1: 2.4800 - output_loss_2: 2.3071 - output_loss_3: 2.0964 - output_loss_4: 1.9328 - output_loss_5: 1.8644 - output_loss_6: 1.8142 - output_loss_7: 1.8258 - output_loss_8: 1.8026    \n",
      "Epoch 9/12\n",
      "75110/75110 [==============================] - 24s - loss: 16.0265 - output_loss_1: 2.4789 - output_loss_2: 2.3056 - output_loss_3: 2.0918 - output_loss_4: 1.9195 - output_loss_5: 1.8480 - output_loss_6: 1.7942 - output_loss_7: 1.8061 - output_loss_8: 1.7825    \n",
      "Epoch 10/12\n",
      "75110/75110 [==============================] - 24s - loss: 15.9431 - output_loss_1: 2.4785 - output_loss_2: 2.3044 - output_loss_3: 2.0849 - output_loss_4: 1.9105 - output_loss_5: 1.8354 - output_loss_6: 1.7775 - output_loss_7: 1.7885 - output_loss_8: 1.7634    \n",
      "Epoch 11/12\n",
      "75110/75110 [==============================] - 24s - loss: 15.8706 - output_loss_1: 2.4780 - output_loss_2: 2.3035 - output_loss_3: 2.0814 - output_loss_4: 1.9013 - output_loss_5: 1.8208 - output_loss_6: 1.7632 - output_loss_7: 1.7748 - output_loss_8: 1.7475    \n",
      "Epoch 12/12\n",
      "75110/75110 [==============================] - 24s - loss: 15.8085 - output_loss_1: 2.4775 - output_loss_2: 2.3021 - output_loss_3: 2.0764 - output_loss_4: 1.8940 - output_loss_5: 1.8108 - output_loss_6: 1.7511 - output_loss_7: 1.7608 - output_loss_8: 1.7359    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47e9442110>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([zeros]+xs, ys, batch_size=64, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "When testing our model, we see that we've attained pretty good results. In the first example, when given ' this is', our model was able to provide a space after predicting the word 'this'. It also knew to start a word after the space. In the second example, when given ' part of', our model was able to give a space before and after accurately predicting the word 'of':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_nexts(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arrs = [np.array(i)[np.newaxis] for i in idxs]\n",
    "    p = model.predict([np.zeros(n_fac)[np.newaxis,:]] + arrs)\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 't', ' ', 's', 'n', ' ']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(' this is')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'p', 'a', 'r', 't', ' ', 'o', 'f']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'o', 'r', 't', ' ', 'o', 'f', ' ']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts(' part of')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "We can also create this model in Keras using its sequential API. To do this, we would need to set the parameter ```return_sequences``` to ```True``` to allow the prediction to step into the next iteration and change our targets into the necessary sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=cs),\n",
    "        SimpleRNN(n_hidden, return_sequences=True, activation='relu', inner_init='identity'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_5 (Embedding)          (None, 8, 42)         3612        embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "simplernn_2 (SimpleRNN)          (None, 8, 256)        76544       embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 8, 86)         22102       simplernn_2[0][0]                \n",
      "====================================================================================================\n",
      "Total params: 102258\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_rnn = np.stack(np.squeeze(xs), axis=1)\n",
    "y_rnn = np.atleast_3d(np.stack(ys, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "75110/75110 [==============================] - 13s - loss: 2.4360    \n",
      "Epoch 2/8\n",
      "75110/75110 [==============================] - 13s - loss: 2.0022    \n",
      "Epoch 3/8\n",
      "75110/75110 [==============================] - 13s - loss: 1.8842    \n",
      "Epoch 4/8\n",
      "75110/75110 [==============================] - 13s - loss: 1.8234    \n",
      "Epoch 5/8\n",
      "75110/75110 [==============================] - 13s - loss: 1.7841    \n",
      "Epoch 6/8\n",
      "75110/75110 [==============================] - 14s - loss: 1.7570    \n",
      "Epoch 7/8\n",
      "75110/75110 [==============================] - 14s - loss: 1.7367    \n",
      "Epoch 8/8\n",
      "75110/75110 [==============================] - 14s - loss: 1.7207    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47e72cf090>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn, y_rnn, batch_size=64, nb_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nexts_keras(inp):\n",
    "    idxs = [char_indices[c] for c in inp]\n",
    "    arr = np.array(idxs)[np.newaxis,:]\n",
    "    p = model.predict(arr)[0]\n",
    "    print(list(inp))\n",
    "    return [chars[np.argmax(o)] for o in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 't', 'h', 'i', 's', ' ', 'i', 's']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 'n', ' ', 's', 's', ' ']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_nexts_keras(' this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "### Building a Stateful Model\n",
    "\n",
    "The models we've built so far (the 4-char, *n*-char, and 2 to *n*-char models) don't really incorporate state as much as we'd like them to. To fix this, we can't train on random batches of data. Instead, when training, we set ```shuffle = false```. Building a stateful model is useful for tasks that require long term memory. To handle long term dependencies, our hidden states will be passed between sequences. We'll start with our initial zero vector input, then pass along our sequence to the next one to always have our hidden state reflect an arbitrarily long dependency. \n",
    "\n",
    "Constructing this model in Keras is simple, just set ```stateful = true``` when creating the model. Then, add batch normalization and use an LSTM layer (briefly introduced in Lesson 5). Remember, normalizing our data rather than directly feeding it into our model improves convergence. Once our model generates a prediction, the data is denormalized to get “real world” results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bs = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        Embedding(vocab_size, n_fac, input_length=cs, batch_input_shape=(bs,8)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(n_hidden, return_sequences=True, stateful=True),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# inputs/outputs must be even multiple of fixed batch size\n",
    "mx = len(x_rnn)//bs*bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, our model’s hidden to hidden layer operations were only applied *n*-1 times. In this stateful model, they are being applied possibly hundreds of thousands of times. Therefore, our network is sensitive to exploding gradients; if the matrix is poorly scaled to even a small degree, a number that's slightly larger than the others would exponentially implode--sending the activations to infinity and destabilizing the network.\n",
    "\n",
    "This instability was corrected using the LSTM model; our network now controls how much state it needs through optimization. It is important to note here that these stateful models compile at a slower rate. This is expected because each sequence is passed through iteratively, making our network harder to parallelize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "75072/75072 [==============================] - 37s - loss: 2.2039    \n",
      "Epoch 2/4\n",
      "75072/75072 [==============================] - 36s - loss: 1.9665    \n",
      "Epoch 3/4\n",
      "75072/75072 [==============================] - 36s - loss: 1.8937    \n",
      "Epoch 4/4\n",
      "75072/75072 [==============================] - 36s - loss: 1.8497    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47caead250>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "75072/75072 [==============================] - 37s - loss: 1.8177    \n",
      "Epoch 2/4\n",
      "75072/75072 [==============================] - 37s - loss: 1.7919    \n",
      "Epoch 3/4\n",
      "75072/75072 [==============================] - 37s - loss: 1.7704    \n",
      "Epoch 4/4\n",
      "75072/75072 [==============================] - 36s - loss: 1.7517    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f47cb9200d0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "75072/75072 [==============================] - 13s - loss: 1.7308    \n",
      "Epoch 2/4\n",
      "75072/75072 [==============================] - 13s - loss: 1.7155    \n",
      "Epoch 3/4\n",
      "75072/75072 [==============================] - 13s - loss: 1.7014    \n",
      "Epoch 4/4\n",
      "75072/75072 [==============================] - 13s - loss: 1.6881    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1773b8d50>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_rnn[:mx], y_rnn[:mx], batch_size=bs, nb_epoch=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Building an RNN in Theano\n",
    "\n",
    "To really understand how we implemented these models in Keras, we will learn how to build a RNN in pure Theano. In the future, we want to build more advanced models. So, it's important to build them from scratch to better understand and debug them on this lower level framework. The same can be done using Tensorflow as the backend.\n",
    "\n",
    "Let's first think of how to construct the task operations we're going to need for our model. To build our layers (input to hidden, hidden to hidden, and hidden to output), we'll need to create our weight matrices and bias vectors from scratch. ```Shared``` is used to tell Theano that the data passing through is manageable by copying it to and from the GPU when necessary. Here, the weights and biases are returned as tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_input = vocab_size\n",
    "n_output = vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def init_wgts(rows, cols): \n",
    "    scale = math.sqrt(2/rows)\n",
    "    return shared(normal(scale=scale, size=(rows, cols)).astype(np.float32))\n",
    "def init_bias(rows): \n",
    "    return shared(np.zeros(rows, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def wgts_and_bias(n_in, n_out): \n",
    "    return init_wgts(n_in, n_out), init_bias(n_out)\n",
    "def id_and_bias(n): \n",
    "    return shared(np.eye(n, dtype=np.float32)), init_bias(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Theano, our variables must be declared before use; no computations are done before our functions are compiled and evaluated. Below, we will declare our matrices, vectors, and scalars and group them in ```all_args```. Next, we can use the functions above to manually initialize the weights and biases to the hidden (```W_h```), input (```W_x```), and output (```W_y```) layers before combining them in a single list (```w_all```). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_inp = T.matrix('inp')\n",
    "t_outp = T.matrix('outp')\n",
    "t_h0 = T.vector('h0')\n",
    "lr = T.scalar('lr')\n",
    "\n",
    "all_args = [t_h0, t_inp, t_outp, lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "W_h = id_and_bias(n_hidden)\n",
    "W_x = wgts_and_bias(n_input, n_hidden)\n",
    "W_y = wgts_and_bias(n_hidden, n_output)\n",
    "w_all = list(chain.from_iterable([W_h, W_x, W_y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we've intialized our inputs, we need to tell Theano what needs to happen in each step (a single forward pass for one character) of our RNN. Our ```step``` function calculates the hidden activations and output. Then, for each scan, we will call this function using the initial values of the outputs, inputs, and all other arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def step(x, h, W_h, b_h, W_x, b_x, W_y, b_y):\n",
    "    # Calculate the hidden activations\n",
    "    h = nnet.relu(T.dot(x, W_x) + b_x + T.dot(h, W_h) + b_h)\n",
    "    # Calculate the output activations\n",
    "    y = nnet.softmax(T.dot(h, W_y) + b_y)\n",
    "    # Return both (the 'Flatten()' is to work around a theano bug)\n",
    "    return h, T.flatten(y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[v_h, v_y], _ = theano.scan(step, sequences=t_inp, \n",
    "                            outputs_info=[t_h0, None], non_sequences=w_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After we've completed one forward pass, we must update our weights by first calculating our loss function. Then, we can perform SGD by storing our updates from each forward pass in a dictionary using ```upd_dict```. We can use Theano functions to do this; categorical cross-entropy will help us calculate error given our step function outputs. ```T.grad``` is then used to find the gradient of our error function before updating our parameters with our learning rate, ```lr```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "error = nnet.categorical_crossentropy(v_y, t_outp).sum()\n",
    "g_all = T.grad(error, w_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def upd_dict(wgts, grads, lr): \n",
    "    return OrderedDict({w: w-g*lr for (w,g) in zip(wgts,grads)})\n",
    "\n",
    "upd = upd_dict(w_all, g_all, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We now have our loss, gradient, and step update functions and are ready to compile them using ```function```, which will give our error function. After each loop, our weights will be updated through this manual form of SGD. The function is then iteratively called for each char from our training data. With each pass, the loss function is calculated and used to update the parameters. Here, Jermey prints the error every thousandth iteration to show how the network is improving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fn = theano.function(all_args, error, updates=upd, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75110, 8, 86), (75110, 8, 86))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh_ys = [to_categorical(o, vocab_size) for o in ys]\n",
    "oh_y_rnn=np.stack(oh_ys, axis=1)\n",
    "\n",
    "oh_xs = [to_categorical(o, vocab_size) for o in xs]\n",
    "oh_x_rnn=np.stack(oh_xs, axis=1)\n",
    "\n",
    "X = oh_x_rnn\n",
    "Y = oh_y_rnn\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:25.158\n",
      "Error:21.460\n",
      "Error:20.904\n",
      "Error:19.906\n",
      "Error:18.812\n",
      "Error:19.281\n",
      "Error:19.061\n",
      "Error:18.505\n",
      "Error:17.926\n",
      "Error:18.193\n",
      "Error:17.430\n",
      "Error:17.620\n",
      "Error:18.409\n",
      "Error:17.324\n",
      "Error:16.794\n",
      "Error:17.751\n",
      "Error:17.322\n",
      "Error:17.197\n",
      "Error:16.828\n",
      "Error:16.665\n",
      "Error:16.520\n",
      "Error:16.429\n",
      "Error:16.676\n",
      "Error:16.197\n",
      "Error:16.835\n",
      "Error:16.559\n",
      "Error:16.144\n",
      "Error:16.295\n",
      "Error:16.285\n",
      "Error:16.415\n",
      "Error:16.727\n",
      "Error:16.439\n",
      "Error:16.713\n",
      "Error:16.358\n",
      "Error:15.976\n",
      "Error:16.674\n",
      "Error:15.975\n",
      "Error:16.386\n",
      "Error:16.063\n",
      "Error:16.259\n",
      "Error:15.349\n",
      "Error:15.734\n",
      "Error:15.723\n",
      "Error:16.061\n",
      "Error:15.973\n",
      "Error:15.910\n",
      "Error:15.658\n",
      "Error:16.064\n",
      "Error:15.978\n",
      "Error:16.110\n",
      "Error:15.217\n",
      "Error:15.606\n",
      "Error:15.022\n",
      "Error:14.843\n",
      "Error:15.621\n",
      "Error:15.380\n",
      "Error:14.701\n",
      "Error:15.558\n",
      "Error:15.103\n",
      "Error:15.032\n",
      "Error:14.977\n",
      "Error:15.397\n",
      "Error:15.325\n",
      "Error:15.122\n",
      "Error:14.672\n",
      "Error:14.806\n",
      "Error:14.297\n",
      "Error:14.682\n",
      "Error:15.249\n",
      "Error:14.802\n",
      "Error:15.153\n",
      "Error:14.698\n",
      "Error:14.446\n",
      "Error:14.519\n",
      "Error:14.449\n"
     ]
    }
   ],
   "source": [
    "err=0.0; l_rate=0.01\n",
    "for i in range(len(X)): \n",
    "    err+=fn(np.zeros(n_hidden), X[i], Y[i], l_rate)\n",
    "    if i % 1000 == 999: \n",
    "        print (\"Error:{:.3f}\".format(err/1000))\n",
    "        err=0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now successfully built an RNN from scratch in Theano! Let's use our model to make predictions. We'll define another Theano function that takes in our hidden layers and input word sequences and converts them to our one-hot encoded outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f_y = theano.function([t_h0, t_inp], v_y, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "act = np.argmax(X[6], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'e', 'n', '?', ' ', 'I', 's']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[indices_char[o] for o in act]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does pretty well when you consider, for example, how it knows to follow a punctuation mark with a space, then begin a new word with a capitalized letter. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
