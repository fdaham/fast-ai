{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 5: NLP\n",
    "\n",
    "In this lesson, we will learn how batch normalization applies to pre-trained CNN's. We'll also continue our dicussion on collaborative filtering to learn how it is used in Natural Language Processing. Finally, we'll briefly introduce Recurrent Neural Networks.\n",
    "\n",
    "## Batch Normalization in Pre-Trained CNNs\n",
    "\n",
    "In Lesson 4, we learned to use batch normalization to increase the training speed and stability of our network to reduce the possibility of overfitting our model without relying on dropout--which is best to avoid since losing data is never optimal. Now, we will learn how batch normalization can be used in a pre-trained network like VGG. Let's first consider the scenario where we use randomly initialized batch normalization parameters. Batch normalization is only added to the dense layers. After each layer, the weights are optimized through gradient descent from the previous activation layer. If we adjust our activation outputs with randomly initialized parameters, the resulting weights would no longer be optimal and may take forever to train back to an optimal state--which defeats the purpose of using a pre-trained network. \n",
    "\n",
    "So, how do we go about correctly using batch normalization in a pre-trained network? First, the batch normalization parameters are shifted/scaled by the standard deviation and mean of the inputs. So, in the first pass through gradient descent, the normalized transformation is reversed and the weights of the outputs are (still) optimal. Starting with our network in a stable state, back propagation then updates the adjustment of the parameters in order to minimize the loss function. This avoids gradient descent falling into a chaotic/unstable state.   \n",
    "\n",
    "Now, going back to...\n",
    "\n",
    "## Collaborative Filtering \n",
    "\n",
    "### Bias Model\n",
    "\n",
    "In the previous lesson, we built a bias model to predict user movie ratings. Given a matrix of user movie ratings, we used randomly initialized embeddings for each user and movie ID as well as bias parameters unique to each ID. A model was then built, where the dot product of the user and movie embeddings were calculated to predict the rating a user would give for any given movie. Gradient descent then optimized these embeddings and biases for each user and movie to minimize the loss function using given (true) user ratings. \n",
    "\n",
    "At this point, it's natural to question why we're using embeddings rather than one-hot encoding each index and mapping them to weight matrices like we did in our image recognition model. These processes are actually the same. Embeddings are $N$-element vectors for $M$ indices, where $N$ and $M$ are natural numbers. To use one-hot-encoding, you just multiply an $N$x$M$ weight matrix with an $M$x$1$ one-hot encoded vector. The resulting learned weights should be the same in either process. The difference is that embedding functions take less computational time to look up indices--making it optimal to use in this model. \n",
    "\n",
    "We can further improve this model by adding regularization. An $L2$ weight regularizer was added to the loss function to minimize the weight values. Our embeddings and biases were created using the following functions:\n",
    "\n",
    "```python\n",
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, W_regularizer=l2(reg))(inp)\n",
    "\n",
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)\n",
    "```\n",
    "\n",
    "This model was built to predict user movie ratings to suggest movies for each user to watch. However, the model can also be used to give us more information on the movies or users themselves. Take, for example, the biases. Our model generates learned biases that--unlike averaging user movie ratings--discards biases from users that rate movies more favorably, critically, etc. From this, we are able to see which movies were truely enjoyed or hated by users. \n",
    "\n",
    "### Latent Factors and PCA\n",
    "\n",
    "Previously, we assumed that the elements of our embeddings, or latent factors, represented user/movie characteristics. We can see this through **Principle Component Analysis (PCA)**. Using PCA, we can take the first three principle components that capture the most information out of the 50 latent factors used in our example. From this, we can attempt to understand what each latent factor is \"measuring\":\n",
    "\n",
    "```python \n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "movie_pca = pca.fit(movie_emb.T).components_\n",
    "```\n",
    "\n",
    "We were able to visualize the PCA on our 50 latent factors by mapping them together. Below, you can see the first principle component mapped against the third. From the bottom left to the top right, we can see the movies seem to be \"measured\" by how classic/intense they are, which leads us to believe our first principle component (mapped on the x-axis) is measuring how classic a movie is while the second (mapped on the y-axis) is measuring movies by their intensity.  \n",
    "\n",
    "![img](https://i.imgur.com/lGnf1JO.png[/img])\n",
    "\n",
    "### Keras' Functional API\n",
    "\n",
    "So far, we've been creating special purpose architectures using Keras' sequential API when we could've been using simpler/more accurate standard neural networks with Keras' functional API. The functional API gives us more control over designing our architectures; you start with your input layer, list every layer between each layer, and call the output with the input from the previous layer (similar to the sequential API, but ordered differently). With a functional API, we are able to add metadata to our CNN models (i.e. the size of our input images). We will continue to use this API in upcoming lessons. \n",
    "\n",
    "## NLP\n",
    "\n",
    "Now, that we've learned how embeddings are used in collaborative filtering, we can can apply them to Natural Language Processing (NLP). Let's start by covering **Sentiment Analysis**, which is used to predict positive or negative sentiment expressed within a given text.\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "Keras comes with the IMDB Sentiment dataset: over 25,000 movie reviews and their respective sentiments. Each review is stored as a vector of word indices in the order of which they are written along with our target outputs: a 1 or 0 for positive or negative sentiment, respectively. Our goal is to predict the sentiment of these words. \n",
    "\n",
    "Let's begin with importing the necessary libraries and setting up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = 'data/imdb/models/'\n",
    "%mkdir -p $model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.pkl\n",
      "2023424/2343108 [========================>.....] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "# vector of words in review\n",
    "idx = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# mapping from id to word\n",
    "idx2word = {v: k for k, v in idx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_full.pkl\n",
      "65560576/65552540 [==============================] - 6s     \n"
     ]
    }
   ],
   "source": [
    "# download reviews\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, there are thousands of unique words used in these reviews, some words being more \"useful\" than others. When a word rarely appears within the dataset and passes through our model, it becomes harder to train and yields results that are less useful than those more frequently used. Therefore, we can truncate the size of our vocabulary to 5000 by setting all rare (barely used) words to the maximum index (```vocab_size-1```). This is easy for us to do as the words are ordered by their frequency. It is also important to note that it doesn’t matter what the index value for the rare word actually is; it's arbitrarily set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "\n",
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also truncate each sequence (or sentence in the review) to a constant length of 500 words (which is twice as big as our mean, so we won't loose too much information) using zero padding. Again, we do this to simplify our dataset. This leaves us with 25,000 reviews, each having a length of 500 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "\n",
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value=0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our set up is now complete and we are ready to start building our models! Let's begin with using a simple neural network.\n",
    "\n",
    "#### Single Hidden Layer Neural Network\n",
    "\n",
    "So, we want to use a single hidden layer neural network to predict sentiment analysis for the IMDB dataset. To do this, we will use embeddings for easy look ups of our movie review IDs. Remember, these IDs are arbitrary; their values have no qualitative significance, they are only used as values for look up. There are 5000 embeddings, or latent factors, with 32 subelements. Why 32? Intuition. If at some point we aren't getting good results from our model, we can change this value. For now, supported through trial and error, we will stick with 32.   \n",
    "\n",
    "Our next step is to flatten our latent factors and their elements, pass them through a single dense layer, and conform our target output using sigmoid as our activation function. We could use softmax instead of sigmoid, but then we would have to change our labels since we're looking for a binary output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 500, 32)       160000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 16000)         0           embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           1600100     flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 100)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             101         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1760201\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 4s - loss: 0.4862 - acc: 0.7262 - val_loss: 0.2927 - val_acc: 0.8769\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 3s - loss: 0.2042 - acc: 0.9228 - val_loss: 0.3170 - val_acc: 0.8685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fffdaa050>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Single Convolutional Layer with Max Pooling (CNN)\n",
    "\n",
    "At this point in the course, we know CNNs are useful when dealing with ordered data. In this case, our input are sentences of ordered words, so a CNN is more likely to give us better results. We will create the simplest possible 1D CNN (as our input is just a 1D vector of words) using a single convolutional layer with max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n",
    "    Dropout(0.2),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Jeremy set the dropout parameter to 20%. This is done to avoid overfitting the specifics of each word’s embedding. The second dropout is used to remove the words (the whole vector effectively). Through trial and error, these values were selected--again, just a lot of playing around with the function settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 12s - loss: 0.5553 - acc: 0.6744 - val_loss: 0.3707 - val_acc: 0.8301\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 12s - loss: 0.3134 - acc: 0.8731 - val_loss: 0.2671 - val_acc: 0.8903\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 12s - loss: 0.2638 - acc: 0.8946 - val_loss: 0.2630 - val_acc: 0.8910\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 11s - loss: 0.2418 - acc: 0.9066 - val_loss: 0.2955 - val_acc: 0.8786\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5ff60fe990>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.save_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv1.load_weights(model_path + 'conv1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "We can now replicate this simple CNN using pre-trained word embeddings. \n",
    "\n",
    "#### Pre-Trained Word Embeddings\n",
    "\n",
    "You should always use pre-trained word embeddings in NLP instead of starting with randomly intialized embeddings because it saves computational time. These pre-trained word models, or latent factors, capture all the useful information about a word and how it behaves through gradient descent. This is an example of **transfer learning**. All we have to do is fine-tune them to fit our model, much like we did when using VGG for image classification.   \n",
    "\n",
    "Pre-trained embeddings are easy to use since words are unique and therefore can only be represented one way, unlike our Imagenet trained weights and filters. Those weights were trained to help us classify images on cats and dogs from the Imagenet dataset. If we were to use other images, it probably wouldn't work as well. With word embeddings, no matter the context, our word will only have one representation.\n",
    "\n",
    "One popular pre-trained word embedding is **glove**, or Global Vectors for Word Representation, which we will use for this lesson. These embeddings were trained from abundantly large corpuses like Wikipedia. These massive unlabeled text dumps (uncased with over 6 million tokens) were used for training, making it an example of **unsupervised learning**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_dataset(dataset):\n",
    "\n",
    "    # see wordvectors.ipynb for info on how these files were\n",
    "    # generated from the original glove data.\n",
    "    md5sums = {'6B.50d': '8e1557d1228decbda7db6dfd81cd9909',\n",
    "               '6B.100d': 'c92dbbeacde2b0384a43014885a60b2c',\n",
    "               '6B.200d': 'af271b46c04b0b2e41a84d8cd806178d',\n",
    "               '6B.300d': '30290210376887dcc6d0a5a6374d8255'}\n",
    "    glove_path = os.path.abspath('data/glove/results')\n",
    "    %mkdir -p $glove_path\n",
    "    return get_file(dataset,\n",
    "                    'http://files.fast.ai/models/glove/' + dataset + '.tgz',\n",
    "                    cache_subdir=glove_path,\n",
    "                    md5_hash=md5sums.get(dataset, None),\n",
    "                    untar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    return (load_array(loc+'.dat'),\n",
    "        pickle.load(open(loc+'_words.pkl','rb')),\n",
    "        pickle.load(open(loc+'_idx.pkl','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://files.fast.ai/models/glove/6B.50d.tgz\n",
      "79560704/80107627 [============================>.] - ETA: 0sUntaring file...\n"
     ]
    }
   ],
   "source": [
    "vecs, words, wordidx = load_vectors(get_glove_dataset('6B.50d'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The glove word IDs and IMBD word IDs are independent of each other and, therefore, have different indices. We can relate them with a simple function that creates an embedding matrix using the indexes from IMDB and the embeddings from glove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    n_fact = vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, n_fact))\n",
    "\n",
    "    for i in range(1,len(emb)):\n",
    "        word = idx2word[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_idx = wordidx[word]\n",
    "            emb[i] = vecs[src_idx]\n",
    "        else:\n",
    "            # If we can't find the word in glove, randomly initialize\n",
    "            emb[i] = normal(scale=0.6, size=(n_fact,))\n",
    "\n",
    "    # This is our \"rare word\" id - we want to randomly initialize\n",
    "    emb[-1] = normal(scale=0.6, size=(n_fact,))\n",
    "    emb/=3\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can now pass our embedding matrix through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, \n",
    "              weights=[emb], trainable=False),\n",
    "    Dropout(0.25),\n",
    "    Convolution1D(64, 5, border_mode='same', activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 12s - loss: 0.6175 - acc: 0.6459 - val_loss: 0.5147 - val_acc: 0.7797\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 12s - loss: 0.5129 - acc: 0.7536 - val_loss: 0.4620 - val_acc: 0.8092\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fe2cb4890>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far, the accuracies beat those from our previous model using random word embeddings! We can further improve on this by fine-tuning the embedding weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 12s - loss: 0.4789 - acc: 0.7755 - val_loss: 0.4522 - val_acc: 0.7919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fe28391d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=1, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'glove50.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Multi-Size CNN\n",
    "\n",
    "When choosing our convolutional filter dimensions, we settle with one optimal filter size (for our image recognition model, we settled on a 3x3 matrix). However, we could build a model that performs convolutions with a range of filter sizes and concatenate their outputs before moving on to the next dense layer. Here, we will implement this method in Keras using the functional API to create a multiple convolutional layer combined with the standard architecture using the sequential API. In this example, we will pass through a size 3 to 6 convolution, each time doing max pooling and flattening. At the end, we'll merge them through concatenation (based off of [Ben Bowles' blog post](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data)).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = [ ] \n",
    "for fsz in range (3, 6): \n",
    "    x = Convolution1D(64, fsz, border_mode='same', activation=\"relu\")(graph_in)\n",
    "    x = MaxPooling1D()(x) \n",
    "    x = Flatten()(x) \n",
    "    convs.append(x)\n",
    "out = Merge(mode=\"concat\")(convs) \n",
    "graph = Model(graph_in, out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential ([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n",
    "    Dropout (0.2),\n",
    "    graph,\n",
    "    Dropout (0.5),\n",
    "    Dense (100, activation=\"relu\"),\n",
    "    Dropout (0.7),\n",
    "    Dense (1, activation='sigmoid')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 30s - loss: 0.5506 - acc: 0.7023 - val_loss: 0.3124 - val_acc: 0.8721\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 30s - loss: 0.3299 - acc: 0.8712 - val_loss: 0.2681 - val_acc: 0.8942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fde7d8a50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 29s - loss: 0.2915 - acc: 0.8849 - val_loss: 0.2583 - val_acc: 0.8952\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 29s - loss: 0.2706 - acc: 0.8963 - val_loss: 0.2558 - val_acc: 0.8984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fdcb3e390>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), nb_epoch=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This more complex architecture has increased our accuracy, as expected.\n",
    "\n",
    "## Introduction to Recurrent Neural Networks (RNNs)\n",
    "\n",
    "How can we build a model that understands how it should predict outputs by following the conventions of the data it was trained by? Specifically, the model would have to keep memory of its previous state to understand what it should expect in the next one. This memory serves to keep track of long-term dependencies, like opening and closing tags in text or reading a street sign in an image letter by letter.\n",
    "\n",
    "We know that CNN APIs require a fixed-size input to generate a fixed-size output through a fixed number of computational steps. We would like our model to be able to handle variable length sequences in order to process variable data structures. RNNs do just that; they allow us to use sequences in the input, output, or both. An example of this can be seen in the diagram below (pulled from [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)):\n",
    "\n",
    "![img](https://i.imgur.com/lBcRdwG.png[/img])\n",
    "\n",
    "RNNs operate on sequences of vectors $x(t)$ and a hidden state vector $h(t)$ at each time step $t$. In the neural network, $A$, each hidden state is a function of the previous hidden state ($h(t-1)$) and the current input $x(t)$. A loop through this network allows information to be passed from one step to the next. The figure below, taken from [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), shows the unrolled version of this loop.\n",
    "\n",
    "![img](https://i.imgur.com/fSmShzx.png[/img])\n",
    "\n",
    "Now that we get the gist of how a simple RNN works, let's use it to predict the next word in a sentence. Our first word would be our first input, $x(0)$. After passing it through $A$, our output is merged with our second word, $x(1)$, which has also been transformed by $A$. This is repeated until we transform the last merged word to generate our output, or the word we predict to follow the preceeding words.   \n",
    "\n",
    "Why is this necessary? Can't we just build a model that takes in all the preceeding words at once? This is where the property of state comes in. After transforming $x(1)$, we have already transformed $x(0)$ twice. Therefore, the second layer of our model not only represents information from the second word, $x(1)$, but also information from the previous word $x(0)$. This way, over time, our model is learning information from these words; information of the current word is dependent on the information from the preceeding words.   \n",
    "\n",
    "We will go more in depth with RNNs in Lesson 6. Another concept we will delve more into in the next lesson are **LSTM (Long Short Term Memory) networks**, which are the most commonly used RNN. They are better at capturing long-term dependencies than simple RNNs and, essentially, only differ by how they compute their hidden state. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
